# ğŸš€ OpenShift Control Plane Node Replacement - UPI Method

## ğŸ“‹ Overview

This document provides a comprehensive procedure for replacing a control plane node in an OpenShift cluster using the User Provisioned Infrastructure (UPI) method. The procedure includes five critical parts:

1. **ğŸ” etcd Backup and Management**
2. **ğŸ’¾ Control Plane Node Replacement with Embedded ISO**
3. **âš™ï¸ BareMetalHost and Machine Configuration**
4. **ğŸ”„ Ceph OSD Recovery**
5. **âœ… Post-Replacement Validation**

---

## ğŸ“‹ Prerequisites

- ğŸ”‘ Administrative access to the OpenShift cluster
- ğŸ–¥ï¸ Access to the physical server or BMC for the target control plane node
- ğŸ’¿ RHCOS ISO image downloaded
- ğŸŒ Network configuration keyfile prepared
- ğŸ” SSH ignition configuration prepared
- ğŸŒ HTTP server accessible from the new node for serving ignition files

### ğŸ“ Required Files

- ğŸ“„ `eno1.nmconnection` - Network configuration keyfile
- ğŸ”‘ `ssh.ign` - SSH ignition configuration
- âš™ï¸ `new_controlplane.ign` - Control plane ignition configuration
- ğŸ’¿ RHCOS ISO image

---

## ğŸ” Part 1: etcd Backup and Preparation

### ğŸ”„ 1.1 Pre-Replacement etcd Backup

Before starting the replacement process, create a comprehensive etcd backup:

```bash
# ğŸ“ 1. Create backup directory
mkdir -p /var/etcd-backup/$(date +%Y%m%d-%H%M%S)
BACKUP_DIR="/var/etcd-backup/$(date +%Y%m%d-%H%M%S)"

# ğŸ’¾ 2. Perform etcd backup
oc debug node/<control-plane-node> -- chroot /host /usr/local/bin/cluster-backup.sh $BACKUP_DIR

# âœ… 3. Verify backup contents
ls -la $BACKUP_DIR/
```

### ğŸ” 1.2 Check etcd Cluster Health

```bash
# ğŸ“Š Check etcd cluster status
oc get etcd -o=jsonpath='{range .items[0].status.conditions[?(@.type=="EtcdMembersAvailable")]}{.message}{"\n"}{end}'

# ğŸ‘¥ List etcd members
oc rsh -n openshift-etcd etcd-<control-plane-node>
etcdctl member list -w table

# â¤ï¸ Check etcd endpoints health
etcdctl endpoint health --cluster -w table
```

### ğŸ¯ 1.3 Identify Target Member

```bash
# ğŸ” Get the etcd member ID for the node being replaced
MEMBER_ID=$(oc rsh -n openshift-etcd etcd-<control-plane-node> etcdctl member list | grep <target-node-name> | cut -d',' -f1)
echo "ğŸ¯ Target etcd member ID: $MEMBER_ID"
```

---

## ğŸ’¾ Part 2: Control Plane Node Replacement (UPI Method)

### ğŸŒ 2.1 Prepare Network Configuration

Create the NetworkManager keyfile for static network configuration:

```bash
# ğŸ“„ Create eno1.nmconnection file
cat > eno1.nmconnection << 'EOF'
[connection]
id=eno1
type=ethernet
interface-name=eno1
autoconnect=true

[ethernet]
mac-address=b8:ce:f6:56:3d:b2

[ipv4]
method=manual
addresses=192.168.24.89/25
gateway=192.168.24.1
dns=192.168.24.80;2600:52:7:24::80
ignore-auto-dns=true
routes=0.0.0.0/0,192.168.24.1

[ipv6]
method=manual
addresses=2600:52:7:24::89/64
gateway=2600:52:7:24::1
dns=192.168.24.80;2600:52:7:24::80
ignore-auto-dns=true
routes=::/0,2600:52:7:24::1
EOF
```

### ğŸ” 2.2 Prepare SSH Ignition Configuration

```bash
# ğŸ”‘ Generate SSH key pair if not exists
ssh-keygen -t rsa -b 4096 -f replacement-key -N "" -C "control-plane-replacement"

# ğŸ“ Create SSH ignition file
cat > ssh.ign << EOF
{
  "ignition": {
    "version": "3.2.0"
  },
  "passwd": {
    "users": [
      {
        "name": "core",
        "sshAuthorizedKeys": [
          "$(cat replacement-key.pub)"
        ]
      },
      {
        "name": "root",
        "sshAuthorizedKeys": [
          "$(cat replacement-key.pub)"
        ]
      }
    ]
  },
  "systemd": {
    "units": [
      {
        "name": "sshd.service",
        "enabled": true
      }
    ]
  }
}
EOF
```

### âš™ï¸ 2.3 Prepare Control Plane Ignition

```bash
# ğŸ“¤ Extract the control plane ignition from existing cluster
oc extract secret/master-user-data --keys=userData -n openshift-machine-api --to=-

# ğŸŒ Serve the ignition file via HTTP server
# ğŸ Option 1: Using Python HTTP server
mkdir -p /var/www/ignition
cp master-user-data /var/www/ignition/new_controlplane.ign
cd /var/www/ignition
python3 -m http.server 9000

# ğŸš€ Option 2: Using nginx or Apache (recommended for production)
```

### ğŸ—‘ï¸ 2.4 Remove the Failed Node

```bash
# âŒ Remove the node from the cluster
oc delete node <target-node-name>

# ğŸ”Œ Remove the etcd member
oc rsh -n openshift-etcd etcd-<healthy-control-plane-node>
etcdctl member remove $MEMBER_ID

# âœ… Verify member removal
etcdctl member list -w table
exit
```

### ğŸ’¿ 2.5 Embed Configuration into ISO

```bash
# ğŸŒ Step 1: Embed network configuration
coreos-installer iso network embed --keyfile eno1.nmconnection rhcos-4.18.1-x86_64-live.x86_64.iso

# ğŸ” Step 2: Embed SSH ignition configuration  
coreos-installer iso ignition embed --ignition-file ssh.ign rhcos-4.18.1-x86_64-live.x86_64.iso

# âœ… Verify embedded configuration
coreos-installer iso show rhcos-4.18.1-x86_64-live.x86_64.iso
```

### ğŸš€ 2.6 Boot and Install the New Node

1. **ğŸ’¿ Boot from the Custom ISO:**
   - ğŸ“€ Mount the modified ISO to the server
   - âš¡ Boot the server from the ISO
   - ğŸŒ The system will automatically configure network and SSH

2. **ğŸ’¾ Install CoreOS to Disk:**

Once the system boots from the ISO and network is configured:

```bash
# ğŸ” SSH into the booted system
ssh -i replacement-key core@192.168.24.89

# ğŸ’¾ Install CoreOS to the target disk
sudo coreos-installer install /dev/disk/by-path/pci-0000:18:00.0-scsi-0:2:1:0 \
    --insecure-ignition \
    --ignition-url=http://192.168.24.80:9000/new_controlplane.ign \
    --insecure-ignition \
    --copy-network

# ğŸ”„ Reboot the system
sudo reboot
```

### âœ… 2.7 Post-Installation Verification

```bash
# â³ Wait for the node to come online
oc get nodes -w

# ğŸ“Š Check node status
oc get node <new-node-name>

# â¤ï¸ Verify etcd cluster health
oc get etcd -o=jsonpath='{range .items[0].status.conditions[?(@.type=="EtcdMembersAvailable")]}{.message}{"\n"}{end}'

# ğŸ‘¥ Check etcd member list
oc rsh -n openshift-etcd etcd-<new-node-name>
etcdctl member list -w table
etcdctl endpoint health --cluster -w table
exit
```

### ğŸ“‹ 2.8 Approve Pending CSRs

```bash
# ğŸ” Check for pending CSRs
oc get csr

# âœ… Approve pending CSRs for the new node
oc get csr -o json | jq -r '.items[] | select(.status == {}) | .metadata.name' | xargs oc adm certificate approve

# ğŸ” Verify all CSRs are approved
oc get csr | grep <new-node-name>
```

---

## âš™ï¸ Part 3: BareMetalHost and Machine Configuration

### ğŸ–¥ï¸ 3.1 Create BareMetalHost (BMH) Resource

After the master node replacement is complete, create a BareMetalHost resource to properly manage the new control plane node:

```yaml
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: master-3-replacement
  namespace: openshift-machine-api
  labels:
    installer.openshift.io/role: control-plane
spec:
  online: true
  bootMACAddress: "b8:ce:f6:56:3d:b2"
  bmc:
    address: "idrac-virtualmedia://192.168.24.157/redfish/v1/Systems/System.Embedded.1"
    credentialsName: master-3-bmc-secret
    disableCertificateVerification: true
  rootDeviceHints:
    deviceName: "/dev/disk/by-path/pci-0000:18:00.0-scsi-0:2:1:0"
  userData:
    name: master-user-data-managed
    namespace: openshift-machine-api
  networkData:
    name: master-3-network-config-secret
    namespace: openshift-machine-api
```

Apply the BareMetalHost resource:

```bash
# ğŸš€ Apply the BareMetalHost resource
oc apply -f bmh-master-3-replacement.yaml
```

### ğŸ¤– 3.2 Create Machine Resource

Create a Machine resource that represents the new control plane node:

```yaml
apiVersion: machine.openshift.io/v1beta1
kind: Machine
metadata:
  name: master-3-replacement
  namespace: openshift-machine-api
  labels:
    machine.openshift.io/cluster-api-cluster: mno-cu-q6pdq
    machine.openshift.io/cluster-api-machine-role: master
    machine.openshift.io/cluster-api-machine-type: master
spec:
  lifecycleHooks: {}
  metadata:
    labels:
      machine.openshift.io/cluster-api-cluster: mno-cu-q6pdq
      machine.openshift.io/cluster-api-machine-role: master
      machine.openshift.io/cluster-api-machine-type: master
  providerSpec:
    value:
      apiVersion: machine.openshift.io/v1alpha1
      kind: BareMetalMachineProviderSpec
      image:
        checksum: "http://192.168.24.80:9000/rhcos-4.18.1-x86_64-metal.x86_64.raw.gz.sha256sum"
        url: "http://192.168.24.80:9000/rhcos-4.18.1-x86_64-metal.x86_64.raw.gz"
      userData:
        name: master-user-data-managed
        namespace: openshift-machine-api
      networkData:
        name: master-3-network-config-secret
        namespace: openshift-machine-api
      hostSelector:
        matchLabels:
          installer.openshift.io/role: control-plane
```

Apply the Machine resource:

```bash
# ğŸš€ Apply the Machine resource
oc apply -f machine-master-3-replacement.yaml
```

### ğŸ†” 3.3 Retrieve ProviderID from Machine

After the Machine resource is created and the node joins the cluster, retrieve the ProviderID:

```bash
# ğŸ” Get the ProviderID from the Machine resource
PROVIDER_ID=$(oc get machine master-3-replacement -n openshift-machine-api -o jsonpath='{.status.providerID}')
echo "âœ… Retrieved ProviderID: $PROVIDER_ID"

# ğŸ”„ Alternative: Get ProviderID directly from the node
NODE_NAME=$(oc get nodes -l node-role.kubernetes.io/control-plane --no-headers | grep master-3 | awk '{print $1}')
NODE_PROVIDER_ID=$(oc get node $NODE_NAME -o jsonpath='{.spec.providerID}')
echo "ğŸ“‹ Node ProviderID: $NODE_PROVIDER_ID"
```

### ğŸ”„ 3.4 Update Node with ProviderID

If the node doesn't have the correct ProviderID, update it:

```bash
# ğŸ” Get the new node name
NEW_NODE_NAME=$(oc get nodes -l node-role.kubernetes.io/control-plane --no-headers | tail -1 | awk '{print $1}')
echo "ğŸ¯ New node name: $NEW_NODE_NAME"

# ğŸ”„ Update the node with the ProviderID from the Machine
oc patch node $NEW_NODE_NAME --type=merge -p "{\"spec\":{\"providerID\":\"$PROVIDER_ID\"}}"

# âœ… Verify the update
oc get node $NEW_NODE_NAME -o jsonpath='{.spec.providerID}'
```

### ğŸ” 3.5 Create Required Secrets

Create the BMC credentials secret:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: master-3-bmc-secret
  namespace: openshift-machine-api
type: Opaque
data:
  username: cm9vdA==  # base64 encoded "root"
  password: Y2FsdmluIA==  # base64 encoded "calvin"
```

Create the network configuration secret:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: master-3-network-config-secret
  namespace: openshift-machine-api
type: Opaque
stringData:
  networkData: |
    network:
      version: 2
      ethernets:
        eno1:
          dhcp4: false
          dhcp6: false
          addresses:
            - 192.168.24.89/25
            - 2600:52:7:24::89/64
          gateway4: 192.168.24.1
          gateway6: 2600:52:7:24::1
          nameservers:
            addresses:
              - 192.168.24.80
              - 2600:52:7:24::80
          match:
            macaddress: b8:ce:f6:56:3d:b2
```

Apply the secrets:

```bash
# ğŸš€ Apply the secrets
oc apply -f master-3-bmc-secret.yaml
oc apply -f master-3-network-config-secret.yaml
```

### âœ… 3.6 Validate BMH and Machine Integration

```bash
# ğŸ–¥ï¸ Check BareMetalHost status
oc get bmh -n openshift-machine-api master-3-replacement

# ğŸ¤– Check Machine status
oc get machine -n openshift-machine-api master-3-replacement

# ğŸ†” Verify the Machine has the correct ProviderID
oc get machine master-3-replacement -n openshift-machine-api -o yaml | grep providerID

# ğŸ”— Check node association
oc get nodes -o wide | grep $(echo $PROVIDER_ID | cut -d'/' -f5)
```

---

## ğŸ”„ Part 4: Ceph OSD Recovery

### ğŸ” 4.1 Identify Affected OSDs

```bash
# ğŸ” Check if the control plane node hosts any OSDs
oc get pods -n openshift-storage -o wide | grep osd | grep <target-node-name>

# ğŸ“Š List all OSDs and their statuses
oc rsh -n openshift-storage $(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name) -- ceph osd tree

# â¤ï¸ Check cluster health
oc rsh -n openshift-storage $(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name) -- ceph health detail
```

### ğŸ—‘ï¸ 4.2 Remove Failed OSDs

```bash
# Get the OSD IDs associated with the failed node
OSD_IDS=$(oc rsh -n openshift-storage $(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name) -- ceph osd tree | grep <target-node-name> | awk '{print $1}')

# For each affected OSD, perform the removal process
for OSD_ID in $OSD_IDS; do
    echo "Processing OSD ID: $OSD_ID"
    
    # Mark OSD as out
    oc rsh -n openshift-storage $(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name) -- ceph osd out $OSD_ID
    
    # Stop the OSD service (if accessible)
    oc rsh -n openshift-storage $(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name) -- ceph tell osd.$OSD_ID stop
    
    # Remove OSD from cluster
    oc rsh -n openshift-storage $(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name) -- ceph osd rm $OSD_ID
    
    # Remove OSD authentication key
    oc rsh -n openshift-storage $(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name) -- ceph auth del osd.$OSD_ID
    
    # Remove OSD from CRUSH map
    oc rsh -n openshift-storage $(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name) -- ceph osd crush remove osd.$OSD_ID
done
```

### ğŸ”§ 4.3 Prepare New OSDs on Replacement Node

```bash
# Wait for the new control plane node to be ready
oc wait --for=condition=Ready node/<new-node-name> --timeout=600s

# Check available disks on the new node
oc debug node/<new-node-name> -- chroot /host lsblk

# If using OCS/ODF operator, add the new node as a storage node
# This assumes the disks will be automatically discovered and used

# Alternatively, manually prepare OSDs if needed:
# 1. Identify available disks for OSD use
NEW_DISK="/dev/sdX"  # Replace with actual disk

# 2. Prepare the OSD using ceph-volume (if direct access is available)
oc debug node/<new-node-name> -- chroot /host ceph-volume lvm prepare --data $NEW_DISK

# 3. Activate all OSDs
oc debug node/<new-node-name> -- chroot /host ceph-volume lvm activate --all
```

### â• 4.4 Add New OSDs to CRUSH Map

```bash
# Get the new OSD IDs that were created
NEW_OSD_IDS=$(oc rsh -n openshift-storage $(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name) -- ceph osd tree | grep <new-node-name> | awk '{print $1}')

# Add each new OSD to the CRUSH map with appropriate weight
for NEW_OSD_ID in $NEW_OSD_IDS; do
    echo "Adding OSD ID: $NEW_OSD_ID to CRUSH map"
    
    # Calculate weight based on disk size (example: 1.0 for 1TB)
    WEIGHT="1.0"  # Adjust based on actual disk size
    
    # Add OSD to CRUSH map
    oc rsh -n openshift-storage $(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name) -- \
        ceph osd crush add osd.$NEW_OSD_ID $WEIGHT root=default host=<new-node-name>
done
```

### ğŸ“Š 4.5 Monitor Cluster Recovery

```bash
# Monitor cluster health during recovery
oc rsh -n openshift-storage $(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name) -- ceph -w

# Check rebalancing progress
oc rsh -n openshift-storage $(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name) -- ceph status

# Verify all OSDs are up and in
oc rsh -n openshift-storage $(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name) -- ceph osd tree

# Check PG status
oc rsh -n openshift-storage $(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name) -- ceph pg stat
```

### âœ… 4.6 Validate ODF Components

```bash
# Check OCS/ODF operator status
oc get csv -n openshift-storage

# Verify storage cluster health
oc get storagecluster -n openshift-storage

# Check all storage pods
oc get pods -n openshift-storage

# Verify storage classes are available
oc get storageclass | grep ceph
```

---

## âœ… Part 5: Post-Replacement Validation

### ğŸ› ï¸ 5.1 Cluster Operators Validation

```bash
# Check cluster operator status
oc get co

# Wait for all operators to be available
oc wait --for=condition=Available --timeout=600s clusteroperators.config.openshift.io --all

# Check for degraded operators
oc get co | grep -v "True.*False.*False"
```

### ğŸ” 5.2 etcd Cluster Validation

```bash
# Verify etcd cluster health
oc rsh -n openshift-etcd etcd-<new-node-name>
etcdctl endpoint health --cluster -w table
etcdctl member list -w table

# Check etcd performance
etcdctl check perf
exit
```

### ğŸ 5.3 Final Cluster Health Validation

```bash
# Final cluster health check
oc get nodes
oc get co
oc get clusterversion

# Verify all components are running
oc get pods --all-namespaces | grep -v Running | grep -v Completed
```

### ğŸš€ 5.4 Application Workload Validation

```bash
# Check pod distribution
oc get pods --all-namespaces -o wide | grep <new-node-name>

# Verify critical workloads
oc get pods -n openshift-etcd
oc get pods -n openshift-kube-apiserver
oc get pods -n openshift-kube-controller-manager
oc get pods -n openshift-kube-scheduler
```

---

## ğŸ”§ Troubleshooting

### âš ï¸ Common Issues and Solutions

#### ğŸŒ 1. Network Configuration Issues

```bash
# ğŸ” Check network interface status
ssh -i replacement-key core@192.168.24.89
ip addr show eno1
ip route show

# ğŸŒ Verify DNS resolution
nslookup api.<cluster-domain>
```

#### ğŸ”¥ 2. Ignition Fetch Issues

```bash
# ğŸ“Š Check ignition service status
sudo systemctl status ignition-firstboot
sudo journalctl -u ignition-firstboot

# ğŸŒ Verify HTTP server accessibility
curl -I http://192.168.24.80:9000/new_controlplane.ign
```

#### ğŸ” 3. etcd Join Issues

```bash
# ğŸ“‹ Check etcd logs
oc logs -n openshift-etcd etcd-<new-node-name>

# ğŸ”— Verify etcd endpoints
oc get endpoints -n openshift-etcd
```

#### ğŸ“œ 4. Certificate Issues

```bash
# ğŸ” Check CSR status
oc get csr | grep Pending

# âœ… Force CSR approval
oc adm certificate approve <csr-name>
```

---

## ğŸš‘ Recovery Procedures

### ğŸ†˜ If Replacement Fails

1. **ğŸ”„ Restore from etcd Backup:**
   ```bash
   # â¹ï¸ Stop etcd on all nodes
   oc patch etcd cluster -p='{"spec":{"managementState":"Unmanaged"}}' --type=merge
   
   # ğŸ”„ Restore from backup
   /usr/local/bin/cluster-restore.sh $BACKUP_DIR
   
   # â–¶ï¸ Restart etcd
   oc patch etcd cluster -p='{"spec":{"managementState":"Managed"}}' --type=merge
   ```

2. **ğŸš¨ Emergency etcd Recovery:**
   ```bash
   # ğŸ“š Follow official disaster recovery procedures
   # ğŸ“– Refer to OpenShift documentation for complete disaster recovery
   ```

---

## ğŸ¯ Best Practices

1. **ğŸ’¾ Always backup etcd before starting replacement**
2. **ğŸ§ª Test the procedure in a non-production environment first**
3. **ğŸŒ Ensure network connectivity to ignition server**
4. **ğŸ“Š Monitor cluster health throughout the process**
5. **ğŸ”„ Have rollback procedures ready**
6. **ğŸ“ Document any deviations from standard procedure**

---

## ğŸ”’ Security Considerations

1. **ğŸ” Use secure ignition server (HTTPS in production)**
2. **ğŸ”‘ Rotate SSH keys after replacement**
3. **ğŸ§¹ Remove temporary HTTP servers**
4. **ğŸ“Š Audit access logs**
5. **ğŸ“‹ Follow organizational security policies**

---

## ğŸ“š References

- ğŸ“– [OpenShift UPI Documentation](https://docs.openshift.com/container-platform/4.18/installing/installing_bare_metal/installing-bare-metal.html)
- ğŸ” [etcd Disaster Recovery](https://docs.openshift.com/container-platform/4.18/backup_and_restore/control_plane_backup_and_restore/disaster_recovery/scenario-2-restoring-cluster-state.html)
- ğŸ’¿ [CoreOS Installer Documentation](https://coreos.github.io/coreos-installer/)
- ğŸ“¦ [OpenShift Data Foundation Documentation](https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/)

---

## ğŸ“‹ Document Information

| **Field** | **Value** |
|-----------|-----------|
| ğŸ“„ **Document Version** | 1.0 |
| ğŸ“… **Last Updated** | $(date) |
| ğŸ”§ **Tested OpenShift Versions** | 4.18, 4.19 |
| ğŸ–¥ï¸ **Target Platform** | Bare Metal UPI |
| ğŸ‘¥ **Author** | OpenShift Infrastructure Team |
| ğŸ¯ **Status** | Production Ready |

---

### ğŸ‰ **Congratulations!** 

You have successfully completed the OpenShift Control Plane Node Replacement using the UPI method! ğŸš€

Your cluster should now be running with the new control plane node properly integrated. 

**Remember to:**
- ğŸ“Š Monitor cluster health for the next 24 hours
- ğŸ”„ Schedule regular etcd backups
- ğŸ“ Update your documentation with any environment-specific notes

**Happy clustering!** ğŸˆ
